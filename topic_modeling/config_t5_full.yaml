dataset:
  name: tweet_topic
  path: cardiffnlp/super_tweeteval

environment:
  num_gpus: 1

reproduce:
    seed: 42

plm:
  model_name: t5
  model_path: t5-small
  optimize: 
    freeze_para: True

train:
  num_epochs: 5
  batch_size: 3
  teacher_forcing: False
  gradient_accumulation_steps: 2

task: generation

generation: # Adding any arguments for generation here.
  parent_config: task
  max_length: 512
  min_length: 5
  temperature: 0.7
  do_sample: False
  top_k: 0
  top_p: 0.9
  repetition_penalty: 1.5
  num_beams: 5
  metric: 
    - sentence_bleu
    - accuracy
    - recall
    - f1
  
learning_setting: full # selecting from "full", "zero_shot", "few_shot"
few_shot:
  parent_config: learning_setting
  few_shot_sampling: sampling_from_train
  
sampling_from_train:
  parent_config: few_shot_sampling
  num_examples_per_label: 100
  also_sample_dev: True
  num_examples_per_label_dev: 100
  seed:
    - 123

template: soft_template
verbalizer: 

soft_template :
  choice: 0
  file_path: ./template.txt
