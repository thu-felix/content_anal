dataset:
  name: imdb
  path: stanfordnlp/imdb

plm:
  model_name: t5
  model_path: t5-small
#   optimize:
#     freeze_para: True
#     lr: 0.0003
#     weight_decay: 0.01
#     scheduler:
#       type:
#       num_warmup_steps: 500

train:
  batch_size: 2

test:
  batch_size: 8

dev:
  batch_size: 8



template: manual_template
verbalizer: manual_verbalizer


manual_template:
  choice: 0
  file_path: ./soft_template.txt


manual_verbalizer:
  choice: 0
  file_path: ./verbalizer.txt
  
environment:
  num_gpus: 1
  cuda_visible_devices:
  local_rank: 0 

learning_setting: few_shot

few_shot:
  parent_config: learning_setting
  few_shot_sampling: sampling_from_train
  
sampling_from_train:
  parent_config: few_shot_sampling
  num_examples_per_label: 10
  also_sample_dev: True
  num_examples_per_label_dev: 10
  seed:
    - 123
    - 456
    - 789
    - 321
    - 654
task: classification
classification:
  parent_config: task
  metric:  # the first one will be the main  to determine checkpoint.
    - accuracy  # whether the higher metric value is better.
    - micro-f1
    - macro-f1
    - precision
    - recall
  loss_function: cross_entropy ## the loss function for classification   
