{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Qy2_bfYHao5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fJBPWCpaHemm"
      },
      "outputs": [],
      "source": [
        "!rm -rf content_anal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDdQfW2RHgVs",
        "outputId": "7861fde7-de84-4fbd-f173-27d438f1b548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'content_anal'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (113/113), done.\u001b[K\n",
            "remote: Total 161 (delta 76), reused 118 (delta 35), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (161/161), 40.47 MiB | 13.65 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/thu-felix/content_anal.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y5sHPhr9Hhh1"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/content_anal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKBwNgThOt1r",
        "outputId": "4a2d6d0e-3aa7-4655-90cd-0d3bbcdc0fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already on 'master'\n",
            "Your branch is up to date with 'origin/master'.\n"
          ]
        }
      ],
      "source": [
        "!git checkout master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYBqICIgOuGR",
        "outputId": "74ecf4bb-46ca-4f29-e2c1-54004dfb3d30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHQdE5UtHjMn",
        "outputId": "b5728dad-0877-4da8-b605-417ffb21b4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'fakenews_dev' set up to track remote branch 'fakenews_dev' from 'origin'.\n",
            "Switched to a new branch 'fakenews_dev'\n"
          ]
        }
      ],
      "source": [
        "!git checkout fakenews_dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGSROuK5HoC3",
        "outputId": "ddcf3fd6-5b16-4e20-c04b-da21c2511ae8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVjUW5D5Hpa7",
        "outputId": "bea28739-6313-475c-b6a6-c8b7ea5e8f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets (from -r requirements.txt (line 1))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
            "Collecting openprompt (from -r requirements.txt (line 3))\n",
            "  Downloading openprompt-1.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.5.1+cu121)\n",
            "Collecting transformers==4.19.0 (from -r requirements.txt (line 5))\n",
            "  Downloading transformers-4.19.0-py3-none-any.whl.metadata (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0->-r requirements.txt (line 5)) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0->-r requirements.txt (line 5)) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0->-r requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0->-r requirements.txt (line 5)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0->-r requirements.txt (line 5)) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0->-r requirements.txt (line 5)) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.19.0->-r requirements.txt (line 5))\n",
            "  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.19.0->-r requirements.txt (line 5)) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (2.2.2)\n",
            "Collecting xxhash (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 1))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r requirements.txt (line 1))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 1)) (3.11.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (1.4.2)\n",
            "Collecting sentencepiece==0.1.96 (from openprompt->-r requirements.txt (line 3))\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting tensorboardX (from openprompt->-r requirements.txt (line 3))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting yacs (from openprompt->-r requirements.txt (line 3))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Collecting rouge==1.0.0 (from openprompt->-r requirements.txt (line 3))\n",
            "  Downloading rouge-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from openprompt->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge==1.0.0->openprompt->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 4)) (1.3.0)\n",
            "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting sentence-transformers (from -r requirements.txt (line 6))\n",
            "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading sentence_transformers-3.1.0-py3-none-any.whl.metadata (23 kB)\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "  Downloading sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "INFO: pip is still looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence_transformers-2.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 6)) (0.20.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.0->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.0->-r requirements.txt (line 5)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.0->-r requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.19.0->-r requirements.txt (line 5)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 6)) (3.5.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->openprompt->-r requirements.txt (line 3)) (4.25.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers->-r requirements.txt (line 6)) (11.0.0)\n",
            "Downloading transformers-4.19.0-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
            "Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=c23ad0290e28dc36756cf2af5b46ecbeea6b5635e0cdfb829bb9ad534ddf5891\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, yacs, xxhash, tensorboardX, rouge, fsspec, dill, multiprocess, transformers, sentence-transformers, datasets, openprompt\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.2.0\n",
            "    Uninstalling sentencepiece-0.2.0:\n",
            "      Successfully uninstalled sentencepiece-0.2.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.2.1\n",
            "    Uninstalling sentence-transformers-3.2.1:\n",
            "      Successfully uninstalled sentence-transformers-3.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 openprompt-1.0.1 rouge-1.0.0 sentence-transformers-2.2.2 sentencepiece-0.1.96 tensorboardX-2.6.2.2 tokenizers-0.12.1 transformers-4.19.0 xxhash-3.5.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKwbVf7rHsKO",
        "outputId": "05fea362-35c2-4539-d8e8-7031c0ef0585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IvF_MpMAHtud"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/content_anal/fakenews')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G2IEflGFHwaR"
      },
      "outputs": [],
      "source": [
        "!mkdir -p logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM9-Q_6AHxwl",
        "outputId": "1cb4ca86-ac60-4743-c6bb-58a810ffe3ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake.csv  True.csv\n"
          ]
        }
      ],
      "source": [
        "ls /content/content_anal/fakenews/dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwnooYExHzDJ",
        "outputId": "36671794-9c74-402e-c2ef-714ceacebbdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-12-13 13:42:09.561587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-13 13:42:09.590179: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-13 13:42:09.595952: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-13 13:42:09.609873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-13 13:42:10.719129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\u001b[032m2024-12-13 13:42:12,316\u001b[0m INFO] config.save_config_to_yaml Config saved as logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/config.yaml\n",
            "[\u001b[032m2024-12-13 13:42:15,942\u001b[0m INFO] reproduciblity.set_seed Global seed set to 100\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
            "[\u001b[032m2024-12-13 13:42:18,088\u001b[0m INFO] prompt_base.from_file using template: {\"placeholder\": \"text_a\"} {\"soft\" : \"Does the news text above real or fake?\"} {\"mask\"} {\"soft\": None}\n",
            "tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (877 > 512). Running this sequence through the model will result in indexing errors\n",
            "tokenizing: 44898it [02:53, 258.32it/s]\n",
            "tokenizing: 44898it [03:00, 248.17it/s]\n",
            "tokenizing: 44898it [03:03, 245.03it/s]\n",
            "[\u001b[032m2024-12-13 13:51:19,658\u001b[0m INFO] cuda.model_to_device Using cuda of single gpu\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "train epoch: 0: 100% 5613/5613 [29:30<00:00,  3.17it/s, loss=0.44]\n",
            "[\u001b[032m2024-12-13 14:20:52,704\u001b[0m INFO] trainer.training_epoch Training epoch 0, num_steps 5613,  avg_loss: 0.8092, total_loss: 4542.1608\n",
            "validation: 100% 22449/22449 [14:48<00:00, 25.25it/s]\n",
            "[\u001b[032m2024-12-13 14:35:41,849\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('accuracy', 0.5522517706802085), ('micro-f1', 0.5522517706802085), ('macro-f1', 0.5048855804847425), ('precision', 0.5684803001876173), ('recall', 0.2546575150581314)])\n",
            "[\u001b[032m2024-12-13 14:35:41,852\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/last.ckpt...\n",
            "[\u001b[032m2024-12-13 14:35:42,437\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/last.ckpt to logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/best.ckpt...\n",
            "[\u001b[032m2024-12-13 14:35:49,343\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
            "train epoch: 1: 100% 5613/5613 [29:34<00:00,  3.16it/s, loss=0.917]\n",
            "[\u001b[032m2024-12-13 15:05:23,380\u001b[0m INFO] trainer.training_epoch Training epoch 1, num_steps 11226,  avg_loss: 0.7497, total_loss: 4207.9532\n",
            "validation: 100% 22449/22449 [14:49<00:00, 25.22it/s]\n",
            "[\u001b[032m2024-12-13 15:20:13,542\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('accuracy', 0.7126598066729031), ('micro-f1', 0.7126598066729031), ('macro-f1', 0.7123057781406993), ('precision', 0.6699393358876118), ('recall', 0.7837699024139703)])\n",
            "[\u001b[032m2024-12-13 15:20:13,545\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/last.ckpt...\n",
            "[\u001b[032m2024-12-13 15:20:19,045\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/last.ckpt to logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/best.ckpt...\n",
            "[\u001b[032m2024-12-13 15:20:19,955\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
            "train epoch: 2: 100% 5613/5613 [29:35<00:00,  3.16it/s, loss=0.794]\n",
            "[\u001b[032m2024-12-13 15:49:55,461\u001b[0m INFO] trainer.training_epoch Training epoch 2, num_steps 16839,  avg_loss: 0.7262, total_loss: 4076.4082\n",
            "validation: 100% 22449/22449 [14:52<00:00, 25.14it/s]\n",
            "[\u001b[032m2024-12-13 16:04:48,546\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('accuracy', 0.7171811661989398), ('micro-f1', 0.7171811661989398), ('macro-f1', 0.7080554150407092), ('precision', 0.6387536204207646), ('recall', 0.9370593453798385)])\n",
            "[\u001b[032m2024-12-13 16:04:48,549\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/last.ckpt...\n",
            "[\u001b[032m2024-12-13 16:04:54,120\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/last.ckpt to logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/best.ckpt...\n",
            "[\u001b[032m2024-12-13 16:04:54,941\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
            "train epoch: 3: 100% 5613/5613 [29:35<00:00,  3.16it/s, loss=0.648]\n",
            "[\u001b[032m2024-12-13 16:34:30,809\u001b[0m INFO] trainer.training_epoch Training epoch 3, num_steps 22452,  avg_loss: 0.7044, total_loss: 3953.8417\n",
            "validation: 100% 22449/22449 [14:50<00:00, 25.21it/s]\n",
            "[\u001b[032m2024-12-13 16:49:21,733\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('accuracy', 0.6980266381576017), ('micro-f1', 0.6980266381576017), ('macro-f1', 0.6823969864537213), ('precision', 0.6175054573726863), ('recall', 0.9641873278236914)])\n",
            "[\u001b[032m2024-12-13 16:49:21,738\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/fakenews_t5-small_mixed_template_manual_verbalizer_1213134212291659/checkpoints/last.ckpt...\n",
            "[\u001b[032m2024-12-13 16:49:26,691\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
            "train epoch: 4: 100% 5613/5613 [29:34<00:00,  3.16it/s, loss=0.418]\n",
            "[\u001b[032m2024-12-13 17:19:01,433\u001b[0m INFO] trainer.training_epoch Training epoch 4, num_steps 28065,  avg_loss: 0.6987, total_loss: 3921.6668\n",
            "validation:  35% 7824/22449 [05:10<09:31, 25.57it/s]"
          ]
        }
      ],
      "source": [
        "!../cli.py --config_yaml config_t5.yaml"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}